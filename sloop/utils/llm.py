"""
LLM è°ƒç”¨å°è£…å·¥å…·

åŸºäº litellm æä¾›ç»Ÿä¸€çš„æ¨¡å‹è°ƒç”¨æ¥å£ï¼Œæ”¯æŒå¤šç§æ¨¡å‹å’Œé…ç½®ã€‚
"""

import sys
from typing import Any, Dict, List, Optional

import litellm

from sloop.utils.logger import logger

# è®¾ç½®æ—¥å¿—


def _get_mock_response(messages: List[Dict[str, Any]], json_mode: bool = False) -> str:
    """
    ç”Ÿæˆæ¨¡æ‹ŸLLMå“åº”ç”¨äºæµ‹è¯•

    å‚æ•°:
        messages: æ¶ˆæ¯åˆ—è¡¨
        json_mode: æ˜¯å¦ä¸ºJSONæ¨¡å¼

    è¿”å›:
        æ¨¡æ‹Ÿå“åº”å­—ç¬¦ä¸²
    """
    if not messages:
        return "æ¨¡æ‹Ÿå“åº”ï¼šç©ºæ¶ˆæ¯åˆ—è¡¨"

    # è·å–æœ€åä¸€æ¡ç”¨æˆ·æ¶ˆæ¯
    last_user_msg = None
    for msg in reversed(messages):
        if msg.get("role") == "user":
            last_user_msg = msg.get("content", "")
            break

    if not last_user_msg:
        return "æ¨¡æ‹Ÿå“åº”ï¼šæœªæ‰¾åˆ°ç”¨æˆ·æ¶ˆæ¯"

    # æ ¹æ®æ¶ˆæ¯å†…å®¹ç”Ÿæˆä¸åŒçš„æ¨¡æ‹Ÿå“åº”
    content_lower = last_user_msg.lower()

    if json_mode:
        # JSONæ¨¡å¼å“åº”
        if "blueprint" in content_lower or "å·¥å…·é“¾" in content_lower:
            return """{
  "intent": "æŸ¥è¯¢å¤©æ°”ä¿¡æ¯å¹¶éªŒè¯å‡†ç¡®æ€§",
  "required_tools": ["alerts_active_zone_zoneid_for_national_weather_service", "forecast_weather_api_for_weatherapi_com", "points_point_stations_for_national_weather_service"],
  "reasoning": "ç”¨æˆ·æƒ³è¦è·å–å¤©æ°”é¢„è­¦ã€æ°”è±¡ç«™æ•°æ®å’Œé¢„æŠ¥ä¿¡æ¯ï¼Œéœ€è¦å¤šä¸ªå·¥å…·åä½œ",
  "is_valid": true
}"""
        elif "tool_call" in content_lower or "å·¥å…·è°ƒç”¨" in content_lower:
            return """{"tool_name": "forecast_weather_api_for_weatherapi_com", "parameters": {"location": "Beijing"}}"""
        elif "decision" in content_lower or "å†³ç­–" in content_lower:
            return "true"
        else:
            return """{"response": "è¿™æ˜¯JSONæ ¼å¼çš„æ¨¡æ‹Ÿå“åº”", "status": "success"}"""

    else:
        # æ™®é€šæ–‡æœ¬å“åº”
        if "å¤©æ°”é¢„è­¦" in content_lower or "weather alert" in content_lower:
            return "ç”¨æˆ·æƒ³è¦æŸ¥è¯¢å¤©æ°”é¢„è­¦ä¿¡æ¯ï¼Œéœ€è¦æ£€æŸ¥æ˜¯å¦æœ‰æ´»è·ƒçš„å¤©æ°”è­¦æŠ¥ã€‚"
        elif "æ°”è±¡ç«™" in content_lower or "weather station" in content_lower:
            return "ç”¨æˆ·è¯¢é—®æ°”è±¡è§‚æµ‹ç«™ä¿¡æ¯ï¼Œåº”è¯¥æŸ¥æ‰¾æœ€è¿‘çš„è§‚æµ‹ç«™ç‚¹ã€‚"
        elif "å¤©æ°”é¢„æŠ¥" in content_lower or "forecast" in content_lower:
            return "ç”¨æˆ·éœ€è¦å¤©æ°”é¢„æŠ¥æ•°æ®ï¼Œå¯ä»¥è°ƒç”¨å¤©æ°”APIè·å–ã€‚"
        elif "blueprint" in content_lower or "è“å›¾" in content_lower:
            return """å¤©æ°”æŸ¥è¯¢å·¥ä½œæµï¼š
1. æ£€æŸ¥å½“å‰åŒºåŸŸæ˜¯å¦æœ‰å¤©æ°”é¢„è­¦
2. æŸ¥æ‰¾æœ€è¿‘çš„æ°”è±¡è§‚æµ‹ç«™
3. è·å–æ°”è±¡ç«™ç‚¹ç½‘æ ¼æ•°æ®
4. è·å–é€å°æ—¶å¤©æ°”é¢„æŠ¥
5. å¯¹æ¯”å•†ä¸šå¤©æ°”APIéªŒè¯å‡†ç¡®æ€§

è¿™ä¸ªå·¥å…·é“¾å¯ä»¥å½¢æˆå®Œæ•´çš„å¤©æ°”ä¿¡æ¯æŸ¥è¯¢æµç¨‹ã€‚"""
        elif "å›å¤" in content_lower or "response" in content_lower:
            return "æ ¹æ®å¤©æ°”æŸ¥è¯¢ç»“æœï¼ŒåŒ—äº¬å¸‚ç›®å‰æ²¡æœ‰å‘å¸ƒå¤©æ°”é¢„è­¦ï¼Œå¤©æ°”çŠ¶å†µè‰¯å¥½ã€‚"
        else:
            return f'æ¨¡æ‹Ÿå“åº”ï¼šå·²æ”¶åˆ°æ‚¨çš„æ¶ˆæ¯"{last_user_msg[:50]}..."'


def completion(
    messages: List[Dict[str, Any]], json_mode: bool = False, **kwargs
) -> str:
    """
    ç»Ÿä¸€çš„LLMè°ƒç”¨æ¥å£

    å‚æ•°:
        messages: æ¶ˆæ¯åˆ—è¡¨ï¼ŒOpenAIæ ¼å¼
        json_mode: æ˜¯å¦å¯ç”¨JSONæ¨¡å¼
        **kwargs: å…¶ä»–å‚æ•°ï¼Œä¼šè¦†ç›–é»˜è®¤è®¾ç½®

    è¿”å›:
        æ¨¡å‹å“åº”å†…å®¹å­—ç¬¦ä¸²

    å¼‚å¸¸:
        å„ç§LLMè°ƒç”¨å¼‚å¸¸ä¼šè¢«æ•è·å¹¶è®°å½•ï¼Œä½†ä¸æŠ›å‡º
    """
    from sloop.config import get_settings

    settings = get_settings()

    # éªŒè¯é…ç½®
    if not settings.validate():
        error_msg = "LLMé…ç½®æ— æ•ˆï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡"
        logger.error(error_msg)
        return f"é…ç½®é”™è¯¯: {error_msg}"

    try:
        # æ£€æŸ¥æ˜¯å¦æ˜¯æµ‹è¯•æ¨¡å¼æˆ–API keyæ— æ•ˆ
        if (
            settings.openai_api_key in ["qwertiasagv", "", None]
            or len(str(settings.openai_api_key)) < 10
        ):
            logger.warning("âš ï¸ æ£€æµ‹åˆ°æ— æ•ˆAPI keyï¼Œä½¿ç”¨æ¨¡æ‹Ÿå“åº”è¿›è¡Œæµ‹è¯•")
            return _get_mock_response(messages, json_mode)

        # å‡†å¤‡è°ƒç”¨å‚æ•°
        call_kwargs = {
            "model": settings.model_name,
            "messages": messages,
            "temperature": settings.temperature,
            "max_tokens": settings.max_tokens,
            "timeout": settings.timeout,
            "api_key": settings.openai_api_key,
        }

        # å¦‚æœè®¾ç½®äº†API base URL
        if settings.openai_api_base:
            call_kwargs["api_base"] = settings.openai_api_base

        # JSONæ¨¡å¼å¤„ç†
        if json_mode:
            # å¯¹äºOpenAIå…¼å®¹çš„API
            if (
                "gpt" in settings.model_name.lower()
                or "openai" in settings.model_name.lower()
            ):
                call_kwargs["response_format"] = {"type": "json_object"}
            # å¯¹äºå…¶ä»–æ¨¡å‹ï¼Œåœ¨ç³»ç»Ÿæ¶ˆæ¯ä¸­æ·»åŠ JSONæŒ‡ä»¤
            elif messages and messages[0].get("role") == "system":
                messages[0]["content"] += "\n\nè¯·ä»¥JSONæ ¼å¼å“åº”ã€‚"
            else:
                # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
                system_msg = {"role": "system", "content": "è¯·ä»¥JSONæ ¼å¼å“åº”ã€‚"}
                messages.insert(0, system_msg)

        # åˆå¹¶ç”¨æˆ·æä¾›çš„é¢å¤–å‚æ•°
        call_kwargs.update(kwargs)

        logger.info(f"è°ƒç”¨LLM: {settings.model_name}, æ¶ˆæ¯æ•°é‡: {len(messages)}")

        # è°ƒç”¨æ¨¡å‹
        response = litellm.completion(**call_kwargs)

        # æå–å“åº”å†…å®¹
        if hasattr(response, "choices") and response.choices:
            content = response.choices[0].message.content
            if content:
                logger.info(f"LLMå“åº”æˆåŠŸï¼Œé•¿åº¦: {len(content)}")
                return content

        # å¦‚æœæ²¡æœ‰å†…å®¹ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
        logger.warning("LLMè¿”å›ç©ºå“åº”")
        return ""

    except Exception as e:
        error_msg = f"LLMè°ƒç”¨å¤±è´¥: {str(e)}"
        logger.error(error_msg, exc_info=True)
        return f"è°ƒç”¨é”™è¯¯: {error_msg}"


def chat_completion(
    prompt: str, system_message: Optional[str] = None, json_mode: bool = False, **kwargs
) -> str:
    """
    ç®€åŒ–çš„èŠå¤©å®Œæˆæ¥å£

    å‚æ•°:
        prompt: ç”¨æˆ·æç¤º
        system_message: ç³»ç»Ÿæ¶ˆæ¯ï¼ˆå¯é€‰ï¼‰
        json_mode: æ˜¯å¦å¯ç”¨JSONæ¨¡å¼
        **kwargs: å…¶ä»–å‚æ•°

    è¿”å›:
        æ¨¡å‹å“åº”å†…å®¹
    """
    messages = []

    # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
    if system_message:
        messages.append({"role": "system", "content": system_message})

    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    messages.append({"role": "user", "content": prompt})

    return completion(messages, json_mode=json_mode, **kwargs)


def validate_llm_config() -> bool:
    """
    éªŒè¯LLMé…ç½®æ˜¯å¦æœ‰æ•ˆ

    è¿”å›:
        é…ç½®æ˜¯å¦æœ‰æ•ˆ
    """
    from sloop.config import get_settings

    settings = get_settings()
    return settings.validate()


def get_supported_models() -> List[str]:
    """
    è·å–litellmæ”¯æŒçš„æ¨¡å‹åˆ—è¡¨ï¼ˆç¤ºä¾‹ï¼‰

    è¿”å›:
        æ”¯æŒçš„æ¨¡å‹åç§°åˆ—è¡¨
    """
    # è¿™é‡Œè¿”å›ä¸€äº›å¸¸è§çš„æ¨¡å‹ä½œä¸ºç¤ºä¾‹
    # å®é™…åº”è¯¥ä»litellmè·å–å®Œæ•´åˆ—è¡¨
    return [
        "gpt-4o",
        "gpt-4o-mini",
        "gpt-3.5-turbo",
        "claude-3-sonnet-20240229",
        "claude-3-haiku-20240307",
        "gemini-pro",
        "deepseek-chat",
        "qwen2-72b-instruct",
    ]


if __name__ == "__main__":
    from sloop.config import get_settings

    logger.info("ğŸ”§ LLM é…ç½®å’Œè°ƒç”¨æµ‹è¯•")
    logger.info("=" * 50)

    # æµ‹è¯•é…ç½®éªŒè¯
    logger.info("ğŸ“‹ é…ç½®çŠ¶æ€:")
    settings = get_settings()
    if settings.validate():
        logger.info("âœ… é…ç½®éªŒè¯é€šè¿‡")
        safe_config = settings.get_safe_display()
        for key, value in safe_config.items():
            logger.info(f"  {key}: {value}")
    else:
        logger.error("âŒ é…ç½®éªŒè¯å¤±è´¥")
        logger.info("\nè¯·è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡:")
        logger.info("  OPENAI_API_KEY=your_api_key_here")
        logger.info("  MODEL_NAME=gpt-4o-mini  # å¯é€‰")
        logger.info("  OPENAI_API_BASE=https://api.openai.com/v1  # å¯é€‰")
        logger.info("  TEMPERATURE=0.7  # å¯é€‰")
        sys.exit(1)

    logger.info("\nğŸ§ª ç®€å•è°ƒç”¨æµ‹è¯•:")

    # æµ‹è¯•ç®€å•è°ƒç”¨ï¼ˆå¦‚æœé…ç½®äº†æœ‰æ•ˆçš„API keyï¼‰
    if settings.openai_api_key and len(settings.openai_api_key) > 10:  # ç®€å•çš„keyéªŒè¯
        try:
            response = chat_completion(
                prompt="è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚", system_message="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ã€‚"
            )

            if response and not response.startswith("è°ƒç”¨é”™è¯¯"):
                logger.info("âœ… LLMè°ƒç”¨æˆåŠŸ")
                logger.info(f"å“åº”é¢„è§ˆ: {response[:100]}...")
            else:
                logger.warning("âš ï¸ LLMè°ƒç”¨å¤±è´¥æˆ–æ— å“åº”")
                logger.warning(f"å“åº”: {response}")

        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•è°ƒç”¨å¤±è´¥: {e}")
    else:
        logger.info("â„¹ï¸ æœªé…ç½®æœ‰æ•ˆçš„API Keyï¼Œè·³è¿‡å®é™…è°ƒç”¨æµ‹è¯•")

    logger.info("\nğŸ“š æ”¯æŒçš„æ¨¡å‹ç¤ºä¾‹:")
    models = get_supported_models()
    for i, model in enumerate(models[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
        logger.info(f"  {i}. {model}")
    if len(models) > 5:
        logger.info(f"  ... è¿˜æœ‰ {len(models) - 5} ä¸ªæ¨¡å‹")

    logger.info("\nâœ… LLMå·¥å…·æµ‹è¯•å®Œæˆ")
