"""
LLM è°ƒç”¨å°è£…å·¥å…·

åŸºäº litellm æä¾›ç»Ÿä¸€çš„æ¨¡å‹è°ƒç”¨æ¥å£ï¼Œæ”¯æŒå¤šç§æ¨¡å‹å’Œé…ç½®ã€‚
"""

import sys
from typing import Any, Dict, List, Optional

import litellm

from sloop.config import get_settings
from sloop.utils.logger import logger

# è®¾ç½®æ—¥å¿—


def completion(
    messages: List[Dict[str, Any]], json_mode: bool = False, **kwargs
) -> str:
    """
    ç»Ÿä¸€çš„LLMè°ƒç”¨æ¥å£

    å‚æ•°:
        messages: æ¶ˆæ¯åˆ—è¡¨ï¼ŒOpenAIæ ¼å¼
        json_mode: æ˜¯å¦å¯ç”¨JSONæ¨¡å¼
        **kwargs: å…¶ä»–å‚æ•°ï¼Œä¼šè¦†ç›–é»˜è®¤è®¾ç½®

    è¿”å›:
        æ¨¡å‹å“åº”å†…å®¹å­—ç¬¦ä¸²

    å¼‚å¸¸:
        å„ç§LLMè°ƒç”¨å¼‚å¸¸ä¼šè¢«æ•è·å¹¶è®°å½•ï¼Œä½†ä¸æŠ›å‡º
    """
    settings = get_settings()

    # éªŒè¯é…ç½®
    if not settings.validate():
        error_msg = "LLMé…ç½®æ— æ•ˆï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå˜é‡"
        logger.error(error_msg)
        return f"é…ç½®é”™è¯¯: {error_msg}"

    try:
        # æ£€æŸ¥API keyæ˜¯å¦æœ‰æ•ˆ
        if not settings.openai_api_key or len(str(settings.openai_api_key)) < 10:
            error_msg = "API keyæ— æ•ˆæˆ–æœªé…ç½®ï¼Œè¯·æ£€æŸ¥OPENAI_API_KEYç¯å¢ƒå˜é‡"
            logger.error(error_msg)
            return f"é…ç½®é”™è¯¯: {error_msg}"

        # å‡†å¤‡è°ƒç”¨å‚æ•°
        call_kwargs = {
            "model": settings.model_name,
            "messages": messages,
            "temperature": settings.temperature,
            "max_tokens": settings.max_tokens,
            "timeout": settings.timeout,
            "api_key": settings.openai_api_key,
        }

        # å¦‚æœè®¾ç½®äº†API base URL
        if settings.openai_api_base:
            call_kwargs["api_base"] = settings.openai_api_base

        # JSONæ¨¡å¼å¤„ç†
        if json_mode:
            # å¯¹äºOpenAIå…¼å®¹çš„API
            if (
                "gpt" in settings.model_name.lower()
                or "openai" in settings.model_name.lower()
            ):
                call_kwargs["response_format"] = {"type": "json_object"}
            # å¯¹äºå…¶ä»–æ¨¡å‹ï¼Œåœ¨ç³»ç»Ÿæ¶ˆæ¯ä¸­æ·»åŠ JSONæŒ‡ä»¤
            elif messages and messages[0].get("role") == "system":
                messages[0]["content"] += "\n\nè¯·ä»¥JSONæ ¼å¼å“åº”ã€‚"
            else:
                # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
                system_msg = {"role": "system", "content": "è¯·ä»¥JSONæ ¼å¼å“åº”ã€‚"}
                messages.insert(0, system_msg)

        # åˆå¹¶ç”¨æˆ·æä¾›çš„é¢å¤–å‚æ•°
        call_kwargs.update(kwargs)

        logger.info(f"è°ƒç”¨LLM: {settings.model_name}, æ¶ˆæ¯æ•°é‡: {len(messages)}")

        # è°ƒç”¨æ¨¡å‹
        response = litellm.completion(**call_kwargs)

        # æå–å“åº”å†…å®¹
        if hasattr(response, "choices") and response.choices:
            content = response.choices[0].message.content
            if content:
                logger.info(f"LLMå“åº”æˆåŠŸï¼Œé•¿åº¦: {len(content)}")
                return content

        # å¦‚æœæ²¡æœ‰å†…å®¹ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²
        logger.warning("LLMè¿”å›ç©ºå“åº”")
        return ""

    except Exception as e:
        error_msg = f"LLMè°ƒç”¨å¤±è´¥: {str(e)}"
        logger.error(error_msg, exc_info=True)
        return f"è°ƒç”¨é”™è¯¯: {error_msg}"


def chat_completion(
    prompt: str, system_message: Optional[str] = None, json_mode: bool = False, **kwargs
) -> str:
    """
    ç®€åŒ–çš„èŠå¤©å®Œæˆæ¥å£

    å‚æ•°:
        prompt: ç”¨æˆ·æç¤º
        system_message: ç³»ç»Ÿæ¶ˆæ¯ï¼ˆå¯é€‰ï¼‰
        json_mode: æ˜¯å¦å¯ç”¨JSONæ¨¡å¼
        **kwargs: å…¶ä»–å‚æ•°

    è¿”å›:
        æ¨¡å‹å“åº”å†…å®¹
    """
    messages = []

    # æ·»åŠ ç³»ç»Ÿæ¶ˆæ¯
    if system_message:
        messages.append({"role": "system", "content": system_message})

    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    messages.append({"role": "user", "content": prompt})

    return completion(messages, json_mode=json_mode, **kwargs)


def validate_llm_config() -> bool:
    """
    éªŒè¯LLMé…ç½®æ˜¯å¦æœ‰æ•ˆ

    è¿”å›:
        é…ç½®æ˜¯å¦æœ‰æ•ˆ
    """
    settings = get_settings()
    return settings.validate()


def get_supported_models() -> List[str]:
    """
    è·å–litellmæ”¯æŒçš„æ¨¡å‹åˆ—è¡¨ï¼ˆç¤ºä¾‹ï¼‰

    è¿”å›:
        æ”¯æŒçš„æ¨¡å‹åç§°åˆ—è¡¨
    """
    # è¿™é‡Œè¿”å›ä¸€äº›å¸¸è§çš„æ¨¡å‹ä½œä¸ºç¤ºä¾‹
    # å®é™…åº”è¯¥ä»litellmè·å–å®Œæ•´åˆ—è¡¨
    return [
        "gpt-4o",
        "gpt-4o-mini",
        "gpt-3.5-turbo",
        "claude-3-sonnet-20240229",
        "claude-3-haiku-20240307",
        "gemini-pro",
        "deepseek-chat",
        "qwen2-72b-instruct",
    ]


if __name__ == "__main__":
    logger.info("ğŸ”§ LLM é…ç½®å’Œè°ƒç”¨æµ‹è¯•")
    logger.info("=" * 50)

    # æµ‹è¯•é…ç½®éªŒè¯
    logger.info("ğŸ“‹ é…ç½®çŠ¶æ€:")
    settings = get_settings()
    if settings.validate():
        logger.info("âœ… é…ç½®éªŒè¯é€šè¿‡")
        safe_config = settings.get_safe_display()
        for key, value in safe_config.items():
            logger.info(f"  {key}: {value}")
    else:
        logger.error("âŒ é…ç½®éªŒè¯å¤±è´¥")
        logger.info("\nè¯·è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡:")
        logger.info("  OPENAI_API_KEY=your_api_key_here")
        logger.info("  MODEL_NAME=gpt-4o-mini  # å¯é€‰")
        logger.info("  OPENAI_API_BASE=https://api.openai.com/v1  # å¯é€‰")
        logger.info("  TEMPERATURE=0.7  # å¯é€‰")
        sys.exit(1)

    logger.info("\nğŸ§ª ç®€å•è°ƒç”¨æµ‹è¯•:")

    # æµ‹è¯•ç®€å•è°ƒç”¨ï¼ˆå¦‚æœé…ç½®äº†æœ‰æ•ˆçš„API keyï¼‰
    if settings.openai_api_key and len(settings.openai_api_key) > 10:  # ç®€å•çš„keyéªŒè¯
        try:
            response = chat_completion(
                prompt="è¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚", system_message="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ã€‚"
            )

            if response and not response.startswith("è°ƒç”¨é”™è¯¯"):
                logger.info("âœ… LLMè°ƒç”¨æˆåŠŸ")
                logger.info(f"å“åº”é¢„è§ˆ: {response[:100]}...")
            else:
                logger.warning("âš ï¸ LLMè°ƒç”¨å¤±è´¥æˆ–æ— å“åº”")
                logger.warning(f"å“åº”: {response}")

        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•è°ƒç”¨å¤±è´¥: {e}")
    else:
        logger.info("â„¹ï¸ æœªé…ç½®æœ‰æ•ˆçš„API Keyï¼Œè·³è¿‡å®é™…è°ƒç”¨æµ‹è¯•")

    logger.info("\nğŸ“š æ”¯æŒçš„æ¨¡å‹ç¤ºä¾‹:")
    models = get_supported_models()
    for i, model in enumerate(models[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ª
        logger.info(f"  {i}. {model}")
    if len(models) > 5:
        logger.info(f"  ... è¿˜æœ‰ {len(models) - 5} ä¸ªæ¨¡å‹")

    logger.info("\nâœ… LLMå·¥å…·æµ‹è¯•å®Œæˆ")
