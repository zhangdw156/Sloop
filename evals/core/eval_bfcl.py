import datetime
import os
import sys

from evalscope import TaskConfig, run_task

# å®šä¹‰ BFCL v3 çš„å®Œæ•´å­é›†åˆ—è¡¨
BFCL_V3_FULL_SUBSETS = [
    "simple",
    "parallel",
    "multiple",
    "parallel_multiple",
    "java",
    "javascript",
    "miss_func",
    "chatable",
    "multi_turn_base",
    "multi_turn_miss_func",
    "multi_turn_miss_param",
    "multi_turn_long_context",
    "long_context",
]


def get_done_subsets(checkpoint_path):
    """è¯»å–å·²å®Œæˆçš„å­é›†åˆ—è¡¨"""
    if not os.path.exists(checkpoint_path):
        return set()
    with open(checkpoint_path, "r", encoding="utf-8") as f:
        lines = [line.strip() for line in f.readlines() if line.strip()]
    return set(lines)


def append_to_checkpoint(checkpoint_path, subset_name):
    """è®°å½•å·²å®Œæˆçš„å­é›†"""
    try:
        with open(checkpoint_path, "a", encoding="utf-8") as f:
            f.write(f"{subset_name}\n")
    except Exception as e:
        print(f"âš ï¸ Warning: Failed to update checkpoint: {e}")


def format_friendly_result(subset_name, raw_result):
    """
    è§£æå¤æ‚çš„ Report å¯¹è±¡ï¼Œç”Ÿæˆäººç±»å¯è¯»çš„æˆç»©å•
    """
    try:
        # 1. è·å–æ ¸å¿ƒ Report å¯¹è±¡
        report = raw_result.get("bfcl_v3")
        if not report:
            return str(raw_result)

        # 2. å‡†å¤‡è¾“å‡ºç¼“å†²åŒº
        lines = []
        lines.append(f"ğŸ“Š Model:   {getattr(report, 'model_name', 'Unknown')}")

        # 3. æ·±å…¥æŒ–æ˜ metrics -> categories -> subsets æ‰¾åˆ°åˆ†æ•°
        found_data = False
        if hasattr(report, "metrics"):
            for metric in report.metrics:
                if hasattr(metric, "categories"):
                    for cat in metric.categories:
                        if hasattr(cat, "subsets"):
                            for sub in cat.subsets:
                                # åªæå–å½“å‰æ­£åœ¨è·‘çš„è¿™ä¸ªå­é›†çš„åˆ†æ•°
                                if sub.name == subset_name:
                                    lines.append(f"ğŸ¯ Subset:  {sub.name}")
                                    lines.append(f"ğŸ”¢ Samples: {sub.num}")
                                    # å°†åˆ†æ•°è½¬æ¢ä¸ºç™¾åˆ†æ¯”æ˜¾ç¤ºï¼Œä¿ç•™2ä½å°æ•°
                                    score_pct = (
                                        sub.score * 100
                                        if sub.score <= 1.0
                                        else sub.score
                                    )
                                    lines.append(
                                        f"ğŸ† Score:   {sub.score:.4f} ({score_pct:.2f}%)"
                                    )
                                    found_data = True

        if found_data:
            return "\n".join(lines)
        else:
            return str(report)

    except Exception as e:
        return f"Error formatting result: {e}\nRaw: {str(raw_result)}"


def append_result_text(filepath, subset_name, raw_result):
    """å°†ç»“æœè¿½åŠ åˆ°æ–‡æœ¬æ–‡ä»¶"""
    try:
        timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # è°ƒç”¨æ ¼å¼åŒ–å‡½æ•°
        formatted_content = format_friendly_result(subset_name, raw_result)

        with open(filepath, "a", encoding="utf-8") as f:
            f.write(f"\n{'=' * 25} {timestamp} {'=' * 25}\n")
            f.write(formatted_content)
            f.write(f"\n{'=' * 72}\n\n")

        print(f"ğŸ’¾ Result saved to {filepath}")
    except Exception as e:
        print(f"âŒ Error saving text result: {e}")


def main():
    # --- 1. è¯»å–åŸºç¡€ç¯å¢ƒå˜é‡ ---
    model_name = os.getenv("EVAL_MODEL_NAME")
    api_url = os.getenv("EVAL_API_URL")
    api_key = os.getenv("EVAL_API_KEY", "EMPTY")
    output_dir = os.getenv("EVAL_OUTPUT_DIR")
    max_tokens = int(os.getenv("EVAL_MAX_TOKENS", "32000"))

    limit_env = os.getenv("EVAL_LIMIT")
    eval_limit = int(limit_env) if limit_env and int(limit_env) > 0 else None

    # --- 2. ç¡®å®šè¦è·‘çš„å­é›† ---
    subset_env = os.getenv("EVAL_SUBSET_LIST", "")
    if subset_env.strip():
        target_subsets = [s.strip() for s in subset_env.split(",")]
    else:
        print("â„¹ï¸ No subset list provided. Using FULL BFCL v3 list.")
        target_subsets = BFCL_V3_FULL_SUBSETS

    print(f"ğŸ”§ Config: Model={model_name}")
    print(f"ğŸ“‚ Output Dir: {output_dir}")

    # --- 3. åˆå§‹åŒ–æ–‡ä»¶è·¯å¾„ ---
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        result_txt_path = os.path.join(output_dir, "evaluation_results.txt")
        checkpoint_path = os.path.join(output_dir, "done_subsets.txt")
        done_subsets = get_done_subsets(checkpoint_path)
    else:
        print("âŒ Error: EVAL_OUTPUT_DIR is not set.")
        sys.exit(1)

    # --- 4. å¾ªç¯æ‰§è¡Œæ¯ä¸ªå­é›† ---
    print(f"ğŸš€ Starting execution loop for {len(target_subsets)} subsets...")

    for i, subset in enumerate(target_subsets):
        print(f"\n[{i + 1}/{len(target_subsets)}] Checking subset: {subset}")

        # [æ–­ç‚¹ç»­ä¼ æ£€æŸ¥]
        if subset in done_subsets:
            print(f"â© Subset [{subset}] already in {checkpoint_path}. Skipping.")
            continue

        try:
            # é…ç½®ä»»åŠ¡
            task_cfg = TaskConfig(
                model=model_name,
                api_url=api_url,
                api_key=api_key,
                eval_type="openai_api",
                datasets=["bfcl_v3"],
                # æŒ‡å®š EvalScope çš„å·¥ä½œç›®å½•
                # è¿™æ ·æ—¥å¿—å’Œä¸´æ—¶æ–‡ä»¶ä¼šç”Ÿæˆåœ¨ output_dir ä¸‹ï¼Œè€Œä¸æ˜¯é»˜è®¤çš„ ./outputs
                work_dir=output_dir,
                eval_batch_size=int(os.getenv("EVAL_BATCH_SIZE", "10")),
                dataset_args={
                    "bfcl_v3": {
                        "subset_list": [subset],
                        "extra_params": {
                            "underscore_to_dot": True,
                            "is_fc_model": True,
                        },
                    }
                },
                generation_config={
                    "temperature": 0,
                    "max_tokens": max_tokens,
                    "parallel_tool_calls": True,
                },
                limit=eval_limit,
            )

            print(f"â–¶ï¸ Running subset: {subset} ...")

            # æ‰§è¡Œè¯„æµ‹
            raw_result = run_task(task_cfg=task_cfg)

            # --- 5. ä¿å­˜ç»“æœ (ä½¿ç”¨ä¼˜åŒ–åçš„æ ¼å¼) ---
            append_result_text(result_txt_path, subset, raw_result)

            # --- 6. æ›´æ–°è¿›åº¦ ---
            append_to_checkpoint(checkpoint_path, subset)
            done_subsets.add(subset)

        except Exception as e:
            err_msg = f"âŒ Error running subset [{subset}]: {e}"
            print(err_msg)
            append_result_text(result_txt_path, subset, err_msg)
            continue

    print("\nâœ… All Subsets Processed.")


if __name__ == "__main__":
    main()
