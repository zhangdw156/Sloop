# -*- coding: utf-8 -*-
import json
import os

import pandas as pd
from tqdm import tqdm
from transformers import AutoTokenizer

# ================= é…ç½®åŒºåŸŸ =================
# æ¨¡å‹è·¯å¾„
MODEL_PATH = "/dfs/data/models/Qwen3-0.6B"

# æ•°æ®é›†è·¯å¾„ (æ ¹æ®ä½ æˆªå›¾ä¸­çš„è·¯å¾„æ¨æµ‹ï¼Œå¦‚æœä¸å¯¹è¯·ä¿®æ”¹)
DATA_PATH = "/dfs/data/datasets/bfcl_v3/data/train-00000-of-00001.parquet"

# æ˜¯å¦åªè®¡ç®— multi_turn ä¸º True çš„æ•°æ®
ONLY_MULTI_TURN = True
# ===========================================


def main():
    print(f"ğŸš€ æ­£åœ¨åŠ è½½ Tokenizer: {MODEL_PATH} ...")
    try:
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
    except Exception as e:
        print(f"âŒ åŠ è½½ Tokenizer å¤±è´¥: {e}")
        return

    print(f"ğŸ“‚ æ­£åœ¨è¯»å–æ•°æ®é›†: {DATA_PATH} ...")
    if not os.path.exists(DATA_PATH):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {DATA_PATH}")
        return

    df = pd.read_parquet(DATA_PATH)

    # ç­›é€‰å¤šè½®å¯¹è¯
    if ONLY_MULTI_TURN and "multi_turn" in df.columns:
        original_len = len(df)
        df = df[df["multi_turn"]].copy()
        print(f"â„¹ï¸ å·²ç­›é€‰ multi_turn=True æ•°æ®: {len(df)} æ¡ (åŸæ•°æ® {original_len} æ¡)")
    else:
        print(f"â„¹ï¸ ä½¿ç”¨å…¨é‡æ•°æ®: {len(df)} æ¡")

    print("mb æ­£åœ¨è®¡ç®— Token æ•°é‡ (è¿™å¯èƒ½éœ€è¦å‡ ç§’é’Ÿ)...")

    # å®šä¹‰è®¡ç®—å‡½æ•°
    def get_token_len(row_turns):
        try:
            # 1. è§£ææ•°æ®æ ¼å¼
            # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå…ˆè½¬ JSON
            data = json.loads(row_turns) if isinstance(row_turns, str) else row_turns

            # å¤„ç† BFCL æ•°æ®é›†å¸¸è§çš„åµŒå¥—åˆ—è¡¨ç»“æ„ [[{...}, {...}]]
            if isinstance(data, list) and len(data) > 0 and isinstance(data[0], list):
                conversation = data[0]
            else:
                conversation = data

            if not isinstance(conversation, list):
                return 0

            # 2. ä½¿ç”¨ apply_chat_template è·å–æœ€ç²¾ç¡®çš„ token æ•°
            # è¿™ä¼šè‡ªåŠ¨åŠ ä¸Š <|im_start|>, <|im_end|>, system prompt ç­‰æ‰€æœ‰ç‰¹æ®Š token
            # tokenize=True ä¼šç›´æ¥è¿”å› token id åˆ—è¡¨
            token_ids = tokenizer.apply_chat_template(
                conversation,
                tokenize=True,
                add_generation_prompt=False,  # è®­ç»ƒ/è¯„æµ‹æ•°æ®é€šå¸¸ä¸éœ€è¦ç”Ÿæˆ prompt
            )
            return len(token_ids)

        except Exception:
            # å¦‚æœ apply_chat_template å¤±è´¥ï¼ˆä¾‹å¦‚æ•°æ®æ ¼å¼ç¼ºå°‘ roleï¼‰ï¼Œå›é€€åˆ°ç²—ç•¥è®¡ç®—
            # print(f"Warning: Template failed, fallback to raw concat. Error: {e}")
            try:
                full_text = ""
                for turn in conversation:
                    content = turn.get("content")
                    if content:
                        full_text += str(content)
                return len(tokenizer.encode(full_text))
            except Exception:
                return 0

    # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
    tqdm.pandas(desc="Processing")
    df["token_count"] = df["turns"].progress_apply(get_token_len)

    # è·å–ç»“æœ
    max_tokens = df["token_count"].max()
    max_idx = df["token_count"].idxmax()

    # è·å–æœ€é•¿é‚£æ¡æ•°æ®çš„è¯¦ç»†ä¿¡æ¯
    longest_row = df.loc[max_idx]

    print("\n" + "=" * 40)
    print("ğŸ“Š ç»Ÿè®¡ç»“æœ (åŸºäº Qwen3 Tokenizer)")
    print("=" * 40)
    print(f"âœ… æœ€é•¿çš„ä¸€æ¡æ•°æ®åŒ…å«: {max_tokens} tokens")
    print(f"ğŸ“ æ•°æ®ç´¢å¼• (Index): {max_idx}")
    print(f"ğŸ“‚ æ‰€å±å­é›† (subset): {longest_row.get('subset', 'N/A')}")
    print("-" * 40)

    # æ£€æŸ¥æ˜¯å¦è¶…è¿‡ 32k
    if max_tokens > 32000:
        print(f"âš ï¸ è­¦å‘Š: æœ€å¤§é•¿åº¦ ({max_tokens}) è¶…è¿‡äº† 32000ï¼")
        print("ğŸ’¡ å»ºè®®: åœ¨è¯„æµ‹è„šæœ¬ä¸­è°ƒå¤§ max_model_len æˆ–å‡å°‘ max_tokens å‚æ•°ã€‚")
    else:
        print(f"ok æœ€å¤§é•¿åº¦ ({max_tokens}) åœ¨ 32000 å®‰å…¨èŒƒå›´å†…ã€‚")
        print(f"   å‰©ä½™ç©ºé—´ (32k - max): {32000 - max_tokens}")


if __name__ == "__main__":
    main()
