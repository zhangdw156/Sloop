{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ecbafe8-9bf7-48ee-a292-638d66efc0cb",
   "metadata": {},
   "source": [
    "## GRPO探索"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3147cc-d3a5-4b57-927b-4efaa77ed4dd",
   "metadata": {},
   "source": [
    "### 准备环境与辅助函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69b0ab0a-9f0e-4f99-9570-a08dbc2b56b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/dfs/data/uv-venv/modelscope/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# 模拟环境设置\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "GROUP_SIZE = 4  # 每组生成 4 个回答 (为了演示方便，实际建议 8)\n",
    "BETA = 0.04     # KL 散度惩罚系数\n",
    "CLIP_EPS = 0.2  # PPO Clip 阈值\n",
    "\n",
    "def get_log_probs(logits, labels):\n",
    "    \"\"\"\n",
    "    计算给定 token 序列的 log probability。\n",
    "    \n",
    "    logits: [Batch, Seq_Len, Vocab]\n",
    "    labels: [Batch, Seq_Len]\n",
    "    \"\"\"\n",
    "    # 1. 对 logits 进行 LogSoftmax\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    # 2. 根据 labels 取出对应的 log_prob\n",
    "    # gather 需要 index 和 src 维度一致\n",
    "    log_probs_gathered = torch.gather(\n",
    "        log_probs, \n",
    "        dim=-1, \n",
    "        index=labels.unsqueeze(-1)\n",
    "    ).squeeze(-1)\n",
    "    \n",
    "    return log_probs_gathered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0ab167-1f5d-48b3-afa3-cfe4d23b83fc",
   "metadata": {},
   "source": [
    "### 核心 GRPO Loss 实现 (包含魔法细节)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0fdbef9-c328-411f-94a8-89e6a704f9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRPOLoss(nn.Module):\n",
    "    def __init__(self, clip_eps=0.2, beta=0.04):\n",
    "        super().__init__()\n",
    "        self.clip_eps = clip_eps\n",
    "        self.beta = beta\n",
    "\n",
    "    def forward(self, \n",
    "                old_log_probs,    # 生成时的 Log Probs (固定值, detach)\n",
    "                new_log_probs,    # 当前训练模型的 Log Probs (带梯度)\n",
    "                ref_log_probs,    # 基座模型的 Log Probs (用于 KL)\n",
    "                rewards,          # 这一组生成的奖励分数 [Batch, Group]\n",
    "                mask              # 掩码，只计算生成部分的 Loss [Batch*Group, Seq_Len]\n",
    "                ):\n",
    "        \n",
    "        # ------------------------------------------------------------------\n",
    "        # 1. 计算优势 Advantage (Group Relative)\n",
    "        # ------------------------------------------------------------------\n",
    "        # rewards shape: [Batch_Size, Group_Size]\n",
    "        mean_rewards = rewards.mean(dim=1, keepdim=True)\n",
    "        std_rewards = rewards.std(dim=1, keepdim=True) + 1e-8 # 防止除0\n",
    "        \n",
    "        # 标准化优势：同组内比较，谁好谁坏\n",
    "        advantages = (rewards - mean_rewards) / std_rewards \n",
    "        \n",
    "        # 展开 specific shape 以匹配 token 维度\n",
    "        # [Batch, Group] -> [Batch * Group, 1]\n",
    "        advantages = advantages.view(-1, 1) \n",
    "        \n",
    "        # ------------------------------------------------------------------\n",
    "        # 2. 计算 Ratio (重要性采样比率)\n",
    "        # ------------------------------------------------------------------\n",
    "        # ratio = exp(new - old) = p_new / p_old\n",
    "        # 注意：这里是 token-level 的 ratio\n",
    "        ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "        \n",
    "        # ------------------------------------------------------------------\n",
    "        # 3. PPO Clip Loss (Policy Loss)\n",
    "        # ------------------------------------------------------------------\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1.0 - self.clip_eps, 1.0 + self.clip_eps) * advantages\n",
    "        \n",
    "        # 我们希望最大化优势，所以 Loss 取负号\n",
    "        policy_loss = -torch.min(surr1, surr2)\n",
    "        \n",
    "        # 应用 Mask：只计算 Completion 部分的 loss，Padding 和 Prompt 部分不计算\n",
    "        policy_loss = (policy_loss * mask).sum() / mask.sum()\n",
    "\n",
    "        # ------------------------------------------------------------------\n",
    "        # 4. KL Divergence Penalty (Regularization)\n",
    "        # ------------------------------------------------------------------\n",
    "        # 近似 KL: log_p - log_ref\n",
    "        # 也就是：模型生成的概率 vs 基座模型生成的概率 的差距\n",
    "        kl_div = torch.exp(ref_log_probs - new_log_probs) - (ref_log_probs - new_log_probs) - 1\n",
    "        # 或者简单的：kl_div = new_log_probs - ref_log_probs\n",
    "        \n",
    "        kl_loss = (kl_div * mask).sum() / mask.sum()\n",
    "        \n",
    "        # ------------------------------------------------------------------\n",
    "        # 5. 总 Loss\n",
    "        # ------------------------------------------------------------------\n",
    "        total_loss = policy_loss + self.beta * kl_loss\n",
    "        \n",
    "        return total_loss, policy_loss, kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8310b34-e8bd-4c91-94eb-9bf8fc422c46",
   "metadata": {},
   "source": [
    "### 完整的训练循环 (Training Loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8e4568-d3ca-4066-be5a-396fd725ffce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. 模型初始化 (LoRA 模式)\n",
    "# ==========================================\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\" # 举例\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    device_map=device\n",
    ")\n",
    "\n",
    "# 配置 LoRA\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    inference_mode=False, \n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    lora_dropout=0.1\n",
    ")\n",
    "model = get_peft_model(base_model, peft_config) # 现在的 model 是 Actor\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "grpo_loss_fn = GRPOLoss(clip_eps=CLIP_EPS, beta=BETA)\n",
    "\n",
    "# ==========================================\n",
    "# 2. 模拟一个 Batch 的 Prompt\n",
    "# ==========================================\n",
    "prompts = [\"计算 123 + 456\", \"查询北京天气\"]\n",
    "# 实际场景：inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "# 为了简化代码，假设 batch_size=1\n",
    "input_text = \"User: Calculate 3+3. Assistant: <think>\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n",
    "prompt_len = inputs.input_ids.shape[1]\n",
    "\n",
    "# ==========================================\n",
    "# 3. 训练循环 (Training Step)\n",
    "# ==========================================\n",
    "model.train()\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# --- Step A: Rollout (采样生成) ---\n",
    "# 这一步不需要梯度，我们是在造数据\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        do_sample=True,          # 必须开启采样！\n",
    "        temperature=0.9,         # 增加熵\n",
    "        top_p=0.95,\n",
    "        max_new_tokens=50,\n",
    "        num_return_sequences=GROUP_SIZE  # 关键：一次生成 G 个\n",
    "    )\n",
    "    # outputs shape: [Group_Size, Seq_Len]\n",
    "    \n",
    "    # 构造 old_log_probs (旧策略产生的概率)\n",
    "    # 此时 model 还没更新，所以就是 model 本身\n",
    "    logits = model(outputs).logits\n",
    "    # Shift logits: 预测下一个 token，所以 logits 往前移，labels 往后对齐\n",
    "    logits = logits[:, :-1, :]\n",
    "    labels = outputs[:, 1:]\n",
    "    \n",
    "    old_log_probs = get_log_probs(logits, labels)\n",
    "    \n",
    "    # 构造 ref_log_probs (基座模型的概率)\n",
    "    # 技巧：在 PeftModel 中，disable_adapter() 就可以变回基座模型\n",
    "    with model.disable_adapter():\n",
    "        ref_logits = model(outputs).logits[:, :-1, :]\n",
    "        ref_log_probs = get_log_probs(ref_logits, labels)\n",
    "\n",
    "# --- Step B: Reward Calculation (打分) ---\n",
    "# 这里你需要接你的 Reward Function\n",
    "# 假设 outputs 的文本中包含 json，我们解析并打分\n",
    "generated_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "rewards = []\n",
    "for text in generated_texts:\n",
    "    # 简单模拟：如果包含 \"6\" 且格式正确，得 1 分，否则 0 分\n",
    "    score = 1.0 if \"6\" in text else 0.0\n",
    "    rewards.append(score)\n",
    "\n",
    "# 转换为 Tensor [1, Group_Size]\n",
    "rewards_tensor = torch.tensor(rewards).to(device).unsqueeze(0) \n",
    "\n",
    "# --- Step C: Forward Pass (当前策略) ---\n",
    "# 这一次需要梯度！\n",
    "# 我们把生成的 outputs 当作输入，再跑一次前向传播\n",
    "outputs_inputs = outputs # [Group_Size, Seq_Len]\n",
    "\n",
    "# 这里的 logits 是带梯度的\n",
    "new_logits = model(outputs_inputs).logits[:, :-1, :]\n",
    "new_labels = outputs_inputs[:, 1:]\n",
    "new_log_probs = get_log_probs(new_logits, new_labels)\n",
    "\n",
    "# --- Step D: Masking (只算生成部分的 Loss) ---\n",
    "# 创建一个 mask，长度等于 labels 的长度\n",
    "mask = torch.zeros_like(new_labels, dtype=torch.float32)\n",
    "# Prompt 部分设为 0，生成部分设为 1\n",
    "# 注意：generate 包含 prompt，所以我们要把 prompt_len 之后的设为 1\n",
    "mask[:, prompt_len-1:] = 1.0 \n",
    "# 如果有 Padding token，也要 mask 掉\n",
    "mask[new_labels == tokenizer.pad_token_id] = 0.0\n",
    "\n",
    "# --- Step E: 计算 Loss ---\n",
    "loss, policy_loss, kl_loss = grpo_loss_fn(\n",
    "    old_log_probs=old_log_probs, # no_grad\n",
    "    new_log_probs=new_log_probs, # grad_fn\n",
    "    ref_log_probs=ref_log_probs, # no_grad\n",
    "    rewards=rewards_tensor,\n",
    "    mask=mask\n",
    ")\n",
    "\n",
    "print(f\"Total Loss: {loss.item():.4f}, Policy Loss: {policy_loss.item():.4f}, KL: {kl_loss.item():.4f}\")\n",
    "\n",
    "# --- Step F: 反向传播 ---\n",
    "loss.backward()\n",
    "\n",
    "# 梯度裁剪 (防止梯度爆炸)\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "optimizer.step()\n",
    "\n",
    "print(\"Step Finished. Parameters updated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modelscope",
   "language": "python",
   "name": "modelscope"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
