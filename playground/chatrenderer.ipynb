{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "154256d5-813f-45f0-93cc-b1de532dc45d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在生成 PDF (修复尺寸版)...\n",
      "✅ 成功保存: complex_chat_example.pdf\n"
     ]
    }
   ],
   "source": [
    "import markdown\n",
    "from markdown.preprocessors import Preprocessor\n",
    "from markdown.inlinepatterns import InlineProcessor\n",
    "from markdown.extensions import Extension\n",
    "import xml.etree.ElementTree as etree\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import base64\n",
    "import re\n",
    "from jinja2 import Template\n",
    "from weasyprint import HTML, CSS\n",
    "from pygments.formatters import HtmlFormatter\n",
    "\n",
    "# --- 0. 配置 Matplotlib 字体 (让公式更协调) ---\n",
    "# 使用 STIX Sans 字体，这是一种无衬线数学字体，和网页正文更搭\n",
    "# 如果报错，可以删掉这一行，回退到默认字体\n",
    "plt.rcParams['mathtext.fontset'] = 'stixsans' \n",
    "\n",
    "# --- 1. 核心渲染函数 ---\n",
    "def latex_to_base64(latex_str, fontsize=14, color='#374151'):\n",
    "    \"\"\"\n",
    "    渲染 LaTeX 为 SVG。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 画布尺寸设得很小，依靠 bbox_inches='tight' 自动撑开\n",
    "        fig = plt.figure(figsize=(0.01, 0.01))\n",
    "        # 渲染文本\n",
    "        fig.text(0, 0, f\"${latex_str}$\", fontsize=fontsize, color=color,\n",
    "                 usetex=False, verticalalignment='center')\n",
    "        \n",
    "        output = io.BytesIO()\n",
    "        # 关键：pad_inches=0.02 减少留白，transparent=True 透明背景\n",
    "        fig.savefig(output, format='svg', bbox_inches='tight', pad_inches=0.02, transparent=True)\n",
    "        plt.close(fig)\n",
    "        \n",
    "        return base64.b64encode(output.getvalue()).decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(f\"渲染失败: {latex_str[:20]}... {e}\")\n",
    "        return None\n",
    "\n",
    "# --- 2. Markdown 扩展部分 (逻辑保持不变) ---\n",
    "class MathBlockPreprocessor(Preprocessor):\n",
    "    def run(self, lines):\n",
    "        text = \"\\n\".join(lines)\n",
    "        pattern = re.compile(r'\\$\\$(.*?)\\$\\$', re.DOTALL)\n",
    "        \n",
    "        def replace_func(match):\n",
    "            latex = match.group(1).strip()\n",
    "            # 块级公式生成得稍微大一点点 (fontsize=16)\n",
    "            img_b64 = latex_to_base64(latex, fontsize=16) \n",
    "            if img_b64:\n",
    "                return f'\\n<div class=\"math-block\"><img src=\"data:image/svg+xml;base64,{img_b64}\" /></div>\\n'\n",
    "            return match.group(0)\n",
    "\n",
    "        new_text = pattern.sub(replace_func, text)\n",
    "        return new_text.split(\"\\n\")\n",
    "\n",
    "class MathInlineProcessor(InlineProcessor):\n",
    "    def handleMatch(self, m, data):\n",
    "        latex = m.group(1)\n",
    "        # 行内公式使用标准字号 (fontsize=14)\n",
    "        img_b64 = latex_to_base64(latex, fontsize=14)\n",
    "        if img_b64:\n",
    "            el = etree.Element(\"img\")\n",
    "            el.set(\"src\", f\"data:image/svg+xml;base64,{img_b64}\")\n",
    "            el.set(\"class\", \"math-inline\")\n",
    "            return el, m.start(0), m.end(0)\n",
    "        return None, None, None\n",
    "\n",
    "class MatplotlibMathExtension(Extension):\n",
    "    def extendMarkdown(self, md):\n",
    "        md.preprocessors.register(MathBlockPreprocessor(md), 'math_block_pre', 50)\n",
    "        md.inlinePatterns.register(MathInlineProcessor(r'(?<!\\$)\\$([^\\$]+)\\$(?!\\$)', md), 'math_inline', 175)\n",
    "\n",
    "# --- 3. 主渲染器 (CSS 重点修改) ---\n",
    "class ChatRenderer:\n",
    "    def _get_syntax_highlighting_css(self):\n",
    "        return HtmlFormatter(style='friendly').get_style_defs('.codehilite')\n",
    "\n",
    "    def _generate_html(self, messages):\n",
    "        pygments_css = self._get_syntax_highlighting_css()\n",
    "\n",
    "        css = f\"\"\"\n",
    "        <style>\n",
    "            @import url('https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&family=JetBrains+Mono&display=swap');\n",
    "            @page {{ size: A4; margin: 0; }}\n",
    "            \n",
    "            body {{\n",
    "                font-family: 'Inter', -apple-system, system-ui, sans-serif;\n",
    "                background-color: #f3f4f6; \n",
    "                padding: 40px 0; \n",
    "                margin: 0;\n",
    "                color: #1f2937;\n",
    "            }}\n",
    "            .chat-container {{\n",
    "                width: 650px; margin: 0 auto; background: white;\n",
    "                border-radius: 12px; padding: 50px;\n",
    "                border: 1px solid #e5e7eb;\n",
    "                box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);\n",
    "            }}\n",
    "            .message {{ margin-bottom: 30px; display: flex; flex-direction: column; page-break-inside: avoid; }}\n",
    "            .role-label {{ font-size: 12px; font-weight: 700; margin-bottom: 8px; text-transform: uppercase; color: #6b7280; letter-spacing: 0.05em; }}\n",
    "            \n",
    "            /* 气泡样式优化 */\n",
    "            .bubble {{ \n",
    "                width: 100%; box-sizing: border-box; \n",
    "                padding: 16px 24px; \n",
    "                border-radius: 8px; \n",
    "                font-size: 15px; \n",
    "                line-height: 1.75; /* 增加行高，让公式不挤 */\n",
    "            }}\n",
    "            \n",
    "            .user .role-label {{ color: #2563eb; }}\n",
    "            .user .bubble {{ background-color: #eff6ff; border: 1px solid #bfdbfe; color: #1e3a8a; }}\n",
    "            \n",
    "            .assistant .role-label {{ color: #4b5563; }}\n",
    "            .assistant .bubble {{ background-color: #f9fafb; border: 1px solid #e5e7eb; color: #374151; }}\n",
    "\n",
    "            /* 代码块 */\n",
    "            {pygments_css}\n",
    "            .codehilite {{ background: white !important; border: 1px solid #e5e7eb; border-radius: 6px; padding: 12px; margin: 12px 0; }}\n",
    "            pre {{ margin: 0; white-space: pre-wrap; font-family: 'JetBrains Mono', monospace; font-size: 13px; }}\n",
    "\n",
    "            /* --- 重点：修复公式大小的 CSS --- */\n",
    "            \n",
    "            /* 1. 行内公式 */\n",
    "            img.math-inline {{ \n",
    "                height: 1.35em; /* 强制高度为字体的1.35倍 */\n",
    "                width: auto;    /* 宽度自适应 */\n",
    "                vertical-align: -0.35em; /* 向下偏移，对齐文字基线 */\n",
    "                margin: 0 2px; /* 左右留一点点空隙 */\n",
    "            }}\n",
    "\n",
    "            /* 2. 块级公式 */\n",
    "            .math-block {{ \n",
    "                display: flex; \n",
    "                justify-content: center; \n",
    "                margin: 16px 0;\n",
    "                width: 100%;\n",
    "            }}\n",
    "            \n",
    "            .math-block img {{ \n",
    "                height: auto; \n",
    "                max-width: 100%; /* 防止撑爆容器 */\n",
    "                /* 如果块级公式还是太大，可以限制最大高度，例如 max-height: 3em; */\n",
    "            }}\n",
    "\n",
    "        </style>\n",
    "        \"\"\"\n",
    "\n",
    "        html_template = \"\"\"\n",
    "        <!DOCTYPE html>\n",
    "        <html>\n",
    "        <head><meta charset=\"UTF-8\">{{ css }}</head>\n",
    "        <body>\n",
    "            <div class=\"chat-container\">\n",
    "                {% for msg in messages %}\n",
    "                <div class=\"message {{ msg.role }}\">\n",
    "                    <div class=\"role-label\">{{ msg.role }}</div>\n",
    "                    <div class=\"bubble\">{{ msg.content_html }}</div>\n",
    "                </div>\n",
    "                {% endfor %}\n",
    "            </div>\n",
    "        </body>\n",
    "        </html>\n",
    "        \"\"\"\n",
    "\n",
    "        math_ext = MatplotlibMathExtension()\n",
    "        processed_msgs = []\n",
    "        for msg in messages:\n",
    "            html_content = markdown.markdown(\n",
    "                msg[\"content\"], \n",
    "                extensions=['fenced_code', 'codehilite', math_ext]\n",
    "            )\n",
    "            processed_msgs.append({\"role\": msg[\"role\"], \"content_html\": html_content})\n",
    "\n",
    "        template = Template(html_template)\n",
    "        return template.render(css=css, messages=processed_msgs)\n",
    "\n",
    "    def save_as_pdf(self, messages, output_path=\"final_fixed_size.pdf\"):\n",
    "        print(\"正在生成 PDF (修复尺寸版)...\")\n",
    "        html_content = self._generate_html(messages)\n",
    "        HTML(string=html_content, base_url='.').write_pdf(output_path)\n",
    "        print(f\"✅ 成功保存: {output_path}\")\n",
    "\n",
    "\n",
    "# --- 测试效果 ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 一个包含 Markdown 标题、列表、数学公式描述和长代码块的复杂数据\n",
    "    long_data = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"我正在学习 Deep Learning，你能详细解释一下 **Transformer** 模型中的 Self-Attention 机制吗？\\n\\n如果可以的话，请给我一个基于 `PyTorch` 的简化代码实现，方便我理解它的矩阵运算过程。\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"\"\"\n",
    "没问题！**Self-Attention (自注意力机制)** 是 Transformer 的核心组件，它解决了 RNN 无法并行计算和长距离依赖的问题。\n",
    "\n",
    "### 1. 数学公式\n",
    "注意力机制的计算公式如下，这是一个**行内公式**示例：$Attention(Q, K, V) = softmax(\\\\frac{QK^T}{\\\\sqrt{d_k}})V$。\n",
    "\n",
    "其中，$d_k$ 是 Key 向量的维度。除以 $\\\\sqrt{d_k}$ 是为了防止点积结果过大导致 Softmax 梯度消失。\n",
    "\n",
    "下面是一个更复杂的**独立公式块**示例：\n",
    "\n",
    "$$\n",
    "\\\\text{MultiHead}(Q, K, V) = \\\\text{Concat}(head_1, ..., head_h)W^O\n",
    "$$\n",
    "\n",
    "其中每个 head 的计算方式为：\n",
    "\n",
    "$$\n",
    "head_i = \\\\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$$\n",
    "\n",
    "### 2. PyTorch 代码实现\n",
    "下面是一个简化版的 Multi-Head Self-Attention 实现。即便代码较长，现在的布局也能完美展示：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        # 定义线性层用来生成 Q, K, V\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        # 获取 batch size\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # 拆分 Heads\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        query = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        # Einsum 矩阵乘法: query * keys\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [query, keys])\n",
    "\n",
    "        # 如果有掩码（Mask），将不需要关注的位置设为负无穷\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        # Softmax 归一化\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        # 加权求和: attention * values\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        return self.fc_out(out)\n",
    "```\n",
    "\n",
    "这个实现涵盖了矩阵拆分、爱因斯坦求和约定 (einsum) 以及 Mask 的处理。 \"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"非常有帮助！代码里的 torch.einsum 用法确实很简洁。既然你提到了 Mask，如果是 Decoder 部分的 Mask，代码会有什么不同吗？\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"\"\" 在 Decoder 中，我们需要用到 Look-ahead Mask (前瞻掩码)。\n",
    "\n",
    "这是为了防止模型在预测当前词时“偷看”到后面的词。我们通常使用一个上三角矩阵来实现：\n",
    "\n",
    "```python\n",
    "# 创建一个上三角矩阵掩码\n",
    "trg_len = 10\n",
    "trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "    N, 1, trg_len, trg_len\n",
    ")\n",
    "\n",
    "# 在 Attention 计算前应用\n",
    "energy = energy.masked_fill(trg_mask == 0, float(\"-1e20\"))\n",
    "```\n",
    "\n",
    "这样，位置 $i$ 的 query 就只能关注到 $0$ 到 $i$ 位置的 key，而无法看到 $i+1$ 之后的信息。\n",
    "\"\"\",\n",
    "        },\n",
    "    ]\n",
    "    renderer = ChatRenderer()\n",
    "    # 使用新数据生成 PDF\n",
    "    renderer.save_as_pdf(long_data, \"complex_chat_example.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e5937c-ced5-480a-97bd-c0fc4fbdf294",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modelscope",
   "language": "python",
   "name": "modelscope"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
