#!/bin/bash

# =========================================================
# 1. è‡ªåŠ¨å‘½åé€»è¾‘
# =========================================================
CURRENT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PARENT_DIR="$(dirname "$CURRENT_DIR")"
GROUP_NAME="$(basename "$CURRENT_DIR")"

if [ -z "$RECIPE_NAME" ]; then
    echo "âŒ Error: RECIPE_NAME is not set. Please run from v1.sh."
    exit 1
fi

export JOB_TIMESTAMP="$(date +%Y%m%d_%H%M)"
export FULL_JOB_NAME="${GROUP_NAME}-${RECIPE_NAME}-${JOB_TIMESTAMP}"

# =========================================================
# 2. åŠ è½½ç¯å¢ƒ
# =========================================================
source "$PARENT_DIR/global_config.sh"

if [ -n "$USE_LOCAL_SWIFT" ]; then
    echo "ğŸ”Œ Activating Local Venv: $SWIFT_ENV_PATH"
    source "$SWIFT_ENV_PATH/bin/activate"
else
    echo "âš¡ï¸ Using System Swift (Default)"
fi

OUTPUT_DIR="$CHECKPOINT_ROOT/$FULL_JOB_NAME"
mkdir -p "$OUTPUT_DIR"

export SWANLAB_LOG_DIR="$OUTPUT_DIR/swanlab_logs"
mkdir -p "$SWANLAB_LOG_DIR"

echo "======================================================="
echo "ğŸš€ Launching Sloop Experiment: $FULL_JOB_NAME"
echo "======================================================="

# =========================================================
# 3. å®šä¹‰å…¨é‡é»˜è®¤å‚æ•°
# =========================================================

# --- A. æ¨¡å‹ä¸æ•°æ® ---
: "${BASE_MODEL:=/dfs/data/models/Qwen3-8B}"
: "${DATA_FILE:=/dfs/data/datasets/LoopTool-23k/LoopTool_grpo_training_data.json}"
: "${MAX_LENGTH:=40960}"

# --- B. è®­ç»ƒåŸºç¡€è¶…å‚ ---
: "${TRAIN_TYPE:=lora}"
: "${EPOCHS:=2}"
: "${LR:=1e-5}"

# [æ˜¾å­˜ä¿å‘½] ä¿æŒä¸º 1
: "${BATCH_SIZE:=1}"

# --- è‡ªåŠ¨è®¡ç®— Accum ---
TARGET_GLOBAL_BATCH=64
GPU_COUNT=$(nvidia-smi -L | wc -l 2>/dev/null || echo 1)
if [ "$GPU_COUNT" -eq 0 ]; then GPU_COUNT=1; fi

CALC_ACCUM=$((TARGET_GLOBAL_BATCH / (BATCH_SIZE * GPU_COUNT)))
if [ "$CALC_ACCUM" -lt 1 ]; then CALC_ACCUM=1; fi

: "${GRAD_ACCUM:=$CALC_ACCUM}"

echo "ğŸ§® Auto-Scaling Config:"
echo "   GPUs: $GPU_COUNT | Local BS: $BATCH_SIZE | Accum: $GRAD_ACCUM"
echo "   => Global Batch Size: $((BATCH_SIZE * GPU_COUNT * GRAD_ACCUM))"

: "${WARMUP_RATIO:=0.05}"
: "${DTYPE:=bfloat16}"
: "${ATTN_IMPL:=flash_attention_2}"

# --- C. LoRA ä¸“å±é…ç½® ---
: "${LORA_RANK:=16}"
: "${LORA_ALPHA:=32}"
: "${TARGET_MODULES:=all-linear}"

# --- D. éªŒè¯ä¸ä¿å­˜ ---
: "${EVAL_STEPS:=200}"  
: "${SAVE_STEPS:=200}"
: "${SAVE_LIMIT:=2}"
: "${LOGGING_STEPS:=5}"

# --- E. ç³»ç»Ÿä¸æ—¥å¿— ---
: "${NUM_WORKERS:=8}"
# [å¿…é¡»å¼€å¯] é•¿æ–‡æœ¬ä¸‹ï¼Œé‡è®¡ç®—æ˜¯å¿…é¡»çš„ï¼Œå¦åˆ™ Deepspeed ä¹Ÿæ•‘ä¸äº† Activation OOM
: "${GRAD_CHECKPOINTING:=true}" 
: "${REPORT_TO:=swanlab}"

# =========================================================
# [ğŸ”¥ æ–°å¢] è‡ªåŠ¨ç”Ÿæˆ DeepSpeed Zero2 Offload é…ç½®æ–‡ä»¶
# =========================================================
DS_CONFIG_PATH="$OUTPUT_DIR/ds_config.json"

cat <<EOF > "$DS_CONFIG_PATH"
{
  "train_batch_size": "auto",
  "train_micro_batch_size_per_gpu": "auto",
  "gradient_accumulation_steps": "auto",
  "gradient_clipping": "auto",
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {
      "device": "cpu",
      "pin_memory": true
    },
    "allgather_partitions": true,
    "allgather_bucket_size": 200000000,
    "reduce_scatter": true,
    "reduce_bucket_size": 200000000,
    "overlap_comm": true,
    "contiguous_gradients": true
  },
  "bf16": {
    "enabled": "auto"
  },
  "fp16": {
    "enabled": "auto"
  }
}
EOF

echo "ğŸ“ DeepSpeed Config generated at: $DS_CONFIG_PATH"

# =========================================================
# 4. æ‰§è¡Œ Swift
# =========================================================

swift sft \
    --model "$BASE_MODEL" \
    --train_type "$TRAIN_TYPE" \
    --dataset "$DATA_FILE" \
    --torch_dtype "$DTYPE" \
    --num_train_epochs "$EPOCHS" \
    --per_device_train_batch_size "$BATCH_SIZE" \
    --per_device_eval_batch_size "$BATCH_SIZE" \
    --gradient_accumulation_steps "$GRAD_ACCUM" \
    --learning_rate "$LR" \
    --lora_rank "$LORA_RANK" \
    --lora_alpha "$LORA_ALPHA" \
    --target_modules "$TARGET_MODULES" \
    --eval_steps "$EVAL_STEPS" \
    --save_steps "$SAVE_STEPS" \
    --save_total_limit "$SAVE_LIMIT" \
    --logging_steps "$LOGGING_STEPS" \
    --max_length "$MAX_LENGTH" \
    --output_dir "$OUTPUT_DIR" \
    --warmup_ratio "$WARMUP_RATIO" \
    --dataloader_num_workers "$NUM_WORKERS" \
    --model_author "$MODEL_AUTHOR" \
    --model_name "$FULL_JOB_NAME" \
    --report_to "$REPORT_TO" \
    --swanlab_project "$PROJECT_NAME" \
    --swanlab_exp_name "$FULL_JOB_NAME" \
    --gradient_checkpointing "$GRAD_CHECKPOINTING" \
    --packing true \
    --attn_impl "$ATTN_IMPL" \
    --deepspeed "$DS_CONFIG_PATH" \
    --device_map ""

echo "âœ… Experiment Finished: $FULL_JOB_NAME"