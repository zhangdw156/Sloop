#!/bin/bash

# =========================================================
# 1. è‡ªåŠ¨å‘½åé€»è¾‘
# =========================================================
CURRENT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PARENT_DIR="$(dirname "$CURRENT_DIR")"
GROUP_NAME="$(basename "$CURRENT_DIR")"

if [ -z "$RECIPE_NAME" ]; then
    echo "âŒ Error: RECIPE_NAME is not set. Please run from v1.sh."
    exit 1
fi

export FULL_JOB_NAME="${GROUP_NAME}-${RECIPE_NAME}"

# =========================================================
# 2. åŠ è½½ç¯å¢ƒ
# =========================================================
source "$PARENT_DIR/global_config.sh"
source "$SWIFT_ENV_PATH/bin/activate"
OUTPUT_DIR="$CHECKPOINT_ROOT/$FULL_JOB_NAME"

echo "======================================================="
echo "ğŸš€ Launching Sloop Experiment: $FULL_JOB_NAME"
echo "======================================================="

# =========================================================
# 3. å®šä¹‰å…¨é‡é»˜è®¤å‚æ•°
# =========================================================

# --- A. æ¨¡å‹ä¸æ•°æ® ---
: "${BASE_MODEL:=/dfs/data/models/Qwen3-8B}"
: "${DATA_FILE:=/dfs/data/datasets/LoopTool-23k/LoopTool_grpo_training_data.json}"
: "${MAX_LENGTH:=40960}"

# [æ–°å¢] éªŒè¯é›†æ¯”ä¾‹: é»˜è®¤ 0.01 (1%)ã€‚å¦‚æœæ˜¯å°æ•°æ®å»ºè®®æ”¹åœ¨ v1.sh é‡Œè®¾ä¸º 0.1
: "${VAL_RATIO:=0.01}"

# --- B. è®­ç»ƒåŸºç¡€è¶…å‚ ---
: "${TRAIN_TYPE:=lora}"
: "${EPOCHS:=4}"
: "${LR:=1e-5}"
: "${BATCH_SIZE:=2}"
: "${EVAL_BATCH_SIZE:=2}"
: "${GRAD_ACCUM:=8}"
: "${WARMUP_RATIO:=0.05}"
: "${DTYPE:=bfloat16}"

# --- C. LoRA ä¸“å±é…ç½® ---
: "${LORA_RANK:=16}"
: "${LORA_ALPHA:=32}"
: "${TARGET_MODULES:=all-linear}"

# --- D. éªŒè¯ä¸ä¿å­˜ ---
# å»ºè®®è°ƒå¤§ä¸€ç‚¹ï¼Œæ¯ 50 æ­¥æµ‹ä¸€æ¬¡æœ‰ç‚¹å¤ªé¢‘ç¹äº†ï¼Œä¼šæ‹–æ…¢è®­ç»ƒ
: "${EVAL_STEPS:=200}"  
: "${SAVE_STEPS:=200}"
: "${SAVE_LIMIT:=2}"
: "${LOGGING_STEPS:=5}"

# --- E. ç³»ç»Ÿä¸æ—¥å¿— ---
: "${NUM_WORKERS:=4}"
: "${GRAD_CHECKPOINTING:=true}"
: "${REPORT_TO:=swanlab}"

# =========================================================
# 4. æ‰§è¡Œ Swift
# =========================================================

mkdir -p "$OUTPUT_DIR"

swift sft \
    --model_id_or_path "$BASE_MODEL" \
    --train_type "$TRAIN_TYPE" \
    --dataset "$DATA_FILE" \
    --dataset_test_ratio "$VAL_RATIO" \
    --torch_dtype "$DTYPE" \
    --num_train_epochs "$EPOCHS" \
    --per_device_train_batch_size "$BATCH_SIZE" \
    --per_device_eval_batch_size "$EVAL_BATCH_SIZE" \
    --gradient_accumulation_steps "$GRAD_ACCUM" \
    --learning_rate "$LR" \
    --lora_rank "$LORA_RANK" \
    --lora_alpha "$LORA_ALPHA" \
    --target_modules "$TARGET_MODULES" \
    --eval_steps "$EVAL_STEPS" \
    --save_steps "$SAVE_STEPS" \
    --save_total_limit "$SAVE_LIMIT" \
    --logging_steps "$LOGGING_STEPS" \
    --max_length "$MAX_LENGTH" \
    --output_dir "$OUTPUT_DIR" \
    --warmup_ratio "$WARMUP_RATIO" \
    --dataloader_num_workers "$NUM_WORKERS" \
    --model_author "$MODEL_AUTHOR" \
    --model_name "$FULL_JOB_NAME" \
    --report_to "$REPORT_TO" \
    --swanlab_project "$PROJECT_NAME" \
    --swanlab_exp_name "$FULL_JOB_NAME" \
    --gradient_checkpointing "$GRAD_CHECKPOINTING"

echo "âœ… Experiment Finished: $FULL_JOB_NAME"