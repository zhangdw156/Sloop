#!/bin/bash

# =========================================================
# Layer 3: Execution Engine (DeepSpeed ç”Ÿæˆ + å¯åŠ¨å‘½ä»¤)
# =========================================================

# 1. å‡†å¤‡è¾“å‡ºç›®å½•
OUTPUT_DIR="$CHECKPOINT_ROOT/$FULL_JOB_NAME"
mkdir -p "$OUTPUT_DIR"
export SWANLAB_LOG_DIR="$OUTPUT_DIR/swanlab_logs"
mkdir -p "$SWANLAB_LOG_DIR"

# 2. è‡ªåŠ¨ç”Ÿæˆ DeepSpeed Config
DS_CONFIG_PATH="$OUTPUT_DIR/ds_config.json"
cat <<EOF > "$DS_CONFIG_PATH"
{
  "train_batch_size": "auto",
  "train_micro_batch_size_per_gpu": "auto",
  "gradient_accumulation_steps": "auto",
  "gradient_clipping": "auto",
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {
      "device": "cpu",
      "pin_memory": true
    },
    "allgather_partitions": true,
    "allgather_bucket_size": 200000000,
    "reduce_scatter": true,
    "reduce_bucket_size": 200000000,
    "overlap_comm": true,
    "contiguous_gradients": true
  },
  "bf16": { "enabled": "auto" },
  "fp16": { "enabled": "auto" }
}
EOF
echo "ğŸ“ DeepSpeed Config generated at: $DS_CONFIG_PATH"

# 3. è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒ (DDP)
# å¦‚æœ GPU_COUNT æ²¡å®šä¹‰ï¼Œé‡æ–°è·å–ä¸€ä¸‹ä½œä¸ºä¿åº•
if [ -z "$GPU_COUNT" ]; then
    GPU_COUNT=$(nvidia-smi -L | wc -l 2>/dev/null || echo 1)
fi
export NPROC_PER_NODE=$GPU_COUNT
export MASTER_PORT=$(($RANDOM + 20000))

# =========================================================
# NEW: åŠ¨æ€æ„å»ºå‚æ•° (Handle LoRA vs Full)
# =========================================================
LORA_ARGS=""
if [ "$TRAIN_TYPE" == "lora" ]; then
    # åªæœ‰åœ¨ lora æ¨¡å¼ä¸‹æ‰ç»„è£…è¿™äº›å‚æ•°
    # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ TARGET_MODULES å†…éƒ¨æ²¡æœ‰ç©ºæ ¼ï¼Œé€šå¸¸æ˜¯ "all-linear" æˆ– "q_proj,v_proj"
    LORA_ARGS="--lora_rank $LORA_RANK --lora_alpha $LORA_ALPHA --target_modules $TARGET_MODULES"
    echo "âš™ï¸  Mode: LoRA (Rank: $LORA_RANK, Alpha: $LORA_ALPHA)"
else
    # å…¨é‡å¾®è°ƒæ¨¡å¼ä¸ºç©º
    LORA_ARGS=""
    echo "âš™ï¸  Mode: Full Fine-Tuning"
fi

echo "ğŸš€ Launching Swift Task..."

# 4. æ‰§è¡Œè®­ç»ƒå‘½ä»¤
# ä¿®æ”¹ç‚¹ï¼šå°† lora_rank ç­‰å‚æ•°æ›¿æ¢ä¸º $LORA_ARGS å˜é‡
# æ³¨æ„ï¼š$LORA_ARGS ä¸è¦åŠ å¼•å·ï¼Œä»¥ä¾¿è®© bash æ­£ç¡®æ‹†åˆ†å‚æ•°
swift sft \
    --model "$BASE_MODEL" \
    --train_type "$TRAIN_TYPE" \
    --dataset $DATA_FILE \
    --torch_dtype "$DTYPE" \
    --num_train_epochs "$EPOCHS" \
    --per_device_train_batch_size "$BATCH_SIZE" \
    --per_device_eval_batch_size "$BATCH_SIZE" \
    --gradient_accumulation_steps "$GRAD_ACCUM" \
    --learning_rate "$LR" \
    $LORA_ARGS \
    --eval_steps "$EVAL_STEPS" \
    --save_steps "$SAVE_STEPS" \
    --save_total_limit "$SAVE_LIMIT" \
    --logging_steps "$LOGGING_STEPS" \
    --max_length "$MAX_LENGTH" \
    --output_dir "$OUTPUT_DIR" \
    --warmup_ratio "$WARMUP_RATIO" \
    --dataloader_num_workers "$NUM_WORKERS" \
    --model_author "$MODEL_AUTHOR" \
    --model_name "$FULL_JOB_NAME" \
    --report_to "$REPORT_TO" \
    --swanlab_project "$PROJECT_NAME" \
    --swanlab_exp_name "$FULL_JOB_NAME" \
    --gradient_checkpointing "$GRAD_CHECKPOINTING" \
    --packing true \
    --attn_impl "$ATTN_IMPL" \
    --deepspeed "$DS_CONFIG_PATH" \
    --loss_scale ${LOSS_SCALE}

echo "âœ… Experiment Finished: $FULL_JOB_NAME"
