#!/bin/bash
# ================================================================
# Layer 2: vLLM Driver Script (Enhanced for LoRA)
# Ë¥üË¥£Êé•Êî∂ÁéØÂ¢ÉÂèòÈáèÔºåÁªÑË£ÖÂëΩ‰ª§ÔºåÂêØÂä® vLLM Server
# ================================================================

# =======================================================
# Environment Setup
# =======================================================

# 1. Á°ÆÂÆöËôöÊãüÁéØÂ¢ÉË∑ØÂæÑ (‰ºòÂÖàÁî® Layer 3 ‰º†ËøõÊù•ÁöÑÔºåÊ≤°ÊúâÂ∞±Áî®ÈªòËÆ§ÁöÑ)
TARGET_VENV=${VENV_PATH:-"/dfs/data/uv-venv/modelscope"}

# 2. Ê£ÄÊü•ÂΩìÂâçÊòØÂê¶Â∑≤ÁªèÂú® venv Èáå‰∫Ü (Èò≤Ê≠¢ÈáçÂ§çÊøÄÊ¥ª)
if [ -z "$VIRTUAL_ENV" ]; then
    if [ -f "$TARGET_VENV/bin/activate" ]; then
        echo "üîå Activating Venv: $TARGET_VENV"
        source "$TARGET_VENV/bin/activate"
    else
        echo "‚ö†Ô∏è  Warning: Venv not found at $TARGET_VENV. Using system python."
    fi
else
    echo "‚úÖ Already in venv: $VIRTUAL_ENV"
fi

# 3. Ê£ÄÊü• uv (ÂèØÈÄâÔºåÂ¶ÇÊûú‰Ω†ÊÉ≥Á°Æ‰øù uv ÂëΩ‰ª§ÂèØÁî®)
if ! command -v uv &> /dev/null; then
    echo "‚ö†Ô∏è  Warning: 'uv' command not found."
fi
# =======================================================

# 1. Ê£ÄÊü•Ê†∏ÂøÉÁéØÂ¢ÉÂèòÈáè
if [ -z "$SERVE_MODEL_PATH" ]; then
    echo "‚ùå Error: SERVE_MODEL_PATH is not set."
    exit 1
fi

# 2. ËÆæÁΩÆÈªòËÆ§ÂÄº
PORT=${SERVE_PORT:-8000}
HOST=${SERVE_HOST:-"0.0.0.0"}
TP_SIZE=${SERVE_TP_SIZE:-1}
MAX_LEN=${SERVE_MAX_LEN:-32768}
GPU_UTIL=${SERVE_GPU_UTIL:-0.90}
DTYPE=${SERVE_DTYPE:-"bfloat16"}
TOOL_PARSER=${SERVE_TOOL_PARSER:-"hermes"}
SWAP_SPACE=${SERVE_SWAP_SPACE:-4}  # [Êñ∞Â¢û] ÈªòËÆ§ 4GB Swap

# 3. ÂáÜÂ§áÊó•ÂøóË∑ØÂæÑ
LOG_DIR="/dfs/data/work/Sloop/serve/logs"
mkdir -p "$LOG_DIR"
LOG_FILE="$LOG_DIR/${SERVE_MODEL_NAME}_${PORT}.log"

# 4. ÊøÄÊ¥ªÁéØÂ¢É
if [ -z "$VIRTUAL_ENV" ]; then
    export VENV_PATH="/dfs/data/uv-venv/modelscope"
    if [ -f "$VENV_PATH/bin/activate" ]; then
        source "$VENV_PATH/bin/activate"
    fi
fi

# 5. [Êñ∞Â¢û] ÊûÑÂª∫ LoRA ÂèÇÊï∞ÈÄªËæë
LORA_ARGS=""
if [ "$SERVE_ENABLE_LORA" == "true" ]; then
    echo "üß© LoRA Enabled."
    LORA_ARGS="--enable-lora --max-lora-rank ${SERVE_MAX_LORA_RANK:-64}"
    
    if [ -n "$SERVE_LORA_MODULES" ]; then
        # Ê≥®ÊÑèÔºöËøôÈáå‰∏çÈúÄË¶ÅÂä†ÂºïÂè∑Ôºå‰ª•‰æøËÆ© shell Ê≠£Á°ÆÊãÜÂàÜÂ§ö‰∏™ module
        LORA_ARGS="$LORA_ARGS --lora-modules $SERVE_LORA_MODULES"
    fi
fi

# 6. ÁªÑË£ÖÂêØÂä®ÂëΩ‰ª§
CMD="uv run --no-project -m vllm.entrypoints.openai.api_server \
    --model $SERVE_MODEL_PATH \
    --served-model-name $SERVE_MODEL_NAME \
    --trust-remote-code \
    --host $HOST \
    --port $PORT \
    --tensor-parallel-size $TP_SIZE \
    --max-model-len $MAX_LEN \
    --gpu-memory-utilization $GPU_UTIL \
    --swap-space $SWAP_SPACE \
    --dtype $DTYPE \
    --enable-auto-tool-choice \
    --tool-call-parser $TOOL_PARSER \
    $LORA_ARGS"  # ËøΩÂä† LoRA ÂèÇÊï∞

echo "========================================================"
echo "üöÄ Starting vLLM Service..."
echo "ü§ñ Base Model: $SERVE_MODEL_NAME"
echo "üß© LoRA:       ${SERVE_ENABLE_LORA:-false}"
echo "üìÇ Path:       $SERVE_MODEL_PATH"
echo "üîå Port:       $PORT"
echo "üìù Log:        $LOG_FILE"
echo "========================================================"

# 7. ÂêØÂä®Ê®°Âºè
if [ "$SERVE_MODE" == "daemon" ]; then
    nohup $CMD > "$LOG_FILE" 2>&1 &
    PID=$!
    echo "‚úÖ vLLM started in background. PID: $PID"
    echo $PID > "$LOG_DIR/${SERVE_MODEL_NAME}_${PORT}.pid"
else
    echo "‚ö†Ô∏è  Running in FOREGROUND mode..."
    $CMD
fi
